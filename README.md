# Actor 3 Delta-Kernel Reader

### Issues with Actor 1
Actor 1 - which is the default reader - is executing sequentially and not valid for delta tables especially the ones with deletion vectors.
### The flow of the code:
1- Table initialization
2- Getting scan files and scan state row
3- Transforming to physical data to logical one and applying the deletion vectors (Where actual reading of columnar batches happens here).

### Multithreading
After collecting the needed data and encapsulates them into objects for transforming the physical data to logical data, for each object a thread is opened for each object (for each parquet file) and the actual transforming and reading of data is done concurrently.

### Performance testing
Performnance testing is done to compare the execution time of actor3 with actor1 and spark through executing with many delta tables with different sizes. Some tables are not tested by actor1 because it takes too much time.

### Validation
Another jar is for reading the delta table and writing it as parquet files and that is for the validator script to compare between files generated by actor3 and spark. The validation purpose is to ensure that all the records written by actor3 are same as records written by spark and there are no duplicates or missing records. This validation is done by generating hash code (using SHA256) for each row in the directory which contains parquet files written by actor3 and doing the same for each row in the directory which contains parquet files written by pyspark and then check that files are identical through the created hashes.
