# Actor 3 Delta-Kernel Reader

### Issues with Actor 1
Actor 1 - which is the default reader - is executing sequentially and not valid for delta tables especially the ones with deletion vectors.
### The flow of the code:
1- Table initialization
2- Getting scan files and scan state row
3- Transforming to physical data to logical one and applying the deletion vectors (Where actual reading of columnar batches happens here).

### Multithreading
After collecting the needed data and encapsulates them into objects for transforming the physical data to logical data, for each object a thread is opened for each object (for each parquet file) and the actual transforming and reading of data is done concurrently.

### Performance testing
Performnance testing is done to compare the execution time of actor3 with actor1 and spark through executing with many delta tables with different sizes. Some tables are not tested by actor1 because it takes too much time.

### Validation
Another jar is for reading the delta table and writing it as parquet files and that is for the validator script to compare between files generated by actor3 and spark. The validation purpose is to ensure that all the records written by actor3 are same as records written by spark and there are no duplicates or missing records. This validation is done by generating hash code (using SHA256) for each row in the directory which contains parquet files written by actor3 and doing the same for each row in the directory which contains parquet files written by pyspark and then check that files are identical through the created hashes.
### Scripts
Scripts exists on the VM in the path: /incorta/scripts
1- actor3-Reader.jar -> actor 3 reading delta table and logging its reading time.
2- actor1.jar -> actor 1 reading delta table and logging its reading time.
3- delta_lake_reader.py -> pyspark scripts reading delta table and logging its reading time.
4- actor3_parquet_writer.jar -> jar writing its data as parquet files in a certain directory path
5- parquet_writer.py -> pyspark script writes table content as parquet files in a certain directory path.
6- parquet_validator.py -> pyspark validator script generates hashcode for each row in actor3 parquet files and same goes to spark parquet files and checks consistency among them.

### Commands
In /incorta/scripts:
Writing delta tables WITHOUT deletion vectors in /incorta/tables:
  spark-submit --packages io.delta:delta-spark_2.13:4.0.0 --driver-memory 8G --executor-memory 8G --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" writeTable.py --rows 1000000 --cols 400 --path "/incorta/tables/largeTable_1000000_400_100" --partitions 100

Writing delta tables WITH deletion vectors in /incorta/tables:
  spark-submit --packages io.delta:delta-spark_2.13:4.0.0 --driver-memory 8G --executor-memory 8G --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" writeTableWithDV.py --rows 1000000 --cols 400 --path "/incorta/tables/largeTable_1000000_400_100" --partitions 100

Reading delta table using ACTOR3 or ACTOR1:
  java -cp <jar_name.jar> <main_file.Main> <delta_table_dir_path>
  
  To specify JVM heap space size:
  java -Xmx<size>g -cp <jar_name.jar> <main_file.Main> <delta_table_dir_path>

Writing delta table using ACTOR3:
  java -cp <jar_name.jar> <main_file.Main> <delta_table_dir_path> <output_dir_path>
  
  To specify JVM heap space size:
  java -Xmx<size>g -cp <jar_name.jar> <main_file.Main> <delta_table_dir_path>

Reading delta table using spark:
  spark-submit --master local[4]  --driver-memory 4G --executor-memory 8G --executor-cores 4  --packages io.delta:delta-spark_2.13:4.0.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" <script.py> <table_path> <logfile.txt>

writing delta table using spark:
    spark-submit --packages io.delta:delta-spark_2.13:4.0.0 <script.py> <output_dir_path>



